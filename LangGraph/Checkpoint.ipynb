{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6681d00d",
   "metadata": {},
   "source": [
    "It's time to add Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72489ed",
   "metadata": {},
   "source": [
    "BUT WAIT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b29c03",
   "metadata": {},
   "source": [
    "We have this Graph maintaining the state and appending to the state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba283bd",
   "metadata": {},
   "source": [
    "Why isn't this handling memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b4132",
   "metadata": {},
   "source": [
    "This is a crucial point for understanding LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959b8dd",
   "metadata": {},
   "source": [
    "A super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of th same super-step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd85bf8",
   "metadata": {},
   "source": [
    "One \"Super-Step\" of the graph represents one invocation of passing messages between agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c77d0",
   "metadata": {},
   "source": [
    "In idiomatic langGraph, you call invoke to run your graph for each super-step, for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13987b70",
   "metadata": {},
   "source": [
    "The reducer handles state updates automatically within one super-step, but not between them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d92584",
   "metadata": {},
   "source": [
    "That is what checkpointing achieves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7897e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver() # This is a checkpoint that saves the state in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee690f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import Annotated\n",
    "from pydantic import BaseModel\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fea5895",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SerperDevTool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tool_search = \u001b[43mSerperDevTool\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'SerperDevTool' is not defined"
     ]
    }
   ],
   "source": [
    "tool_search = SerperDevTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d7a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_search]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2a8fa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m graph_builder = StateGraph(State)\n\u001b[32m      4\u001b[39m llm = ChatOpenAI(model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m, temperature=\u001b[32m0.7\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m llm_with_tools = llm.bind_tools(\u001b[43mtools\u001b[49m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchatbot\u001b[39m(state: State):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(state)\n",
      "\u001b[31mNameError\u001b[39m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "def chatbot(state: State):\n",
    "    print(state)\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "\n",
    "graph_builder.add_conditional_edges(\"chatbot\", tools_condition, \"tools\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "\n",
    "graph = graph_builder.compile(checkpoint=memory)\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac4caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Jeevan\\Agentic_AI\\LangGraph_Langchain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "config = {\"configurable\": {\"thread_id\": 1}}\n",
    "# thread_id: This is the thread id for the graph. It is used to identify the thread in the graph, (it is conversational thread)\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    result = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}, config=config)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69977bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
