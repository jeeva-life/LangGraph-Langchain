{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c08b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfde4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab3aea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.langchain.com/breakoutagents\",\n",
    "    \"https://blog.langchain.com/langchain-state-of-ai-2024\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83775421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://www.langchain.com/breakoutagents', 'title': 'Breakout Agentic Apps', 'description': 'Dive into the stories of companies pushing the boundaries of AI agents. Learn \"why\" and \"how\" they made specific architecture, UX, prompt engineering, and evaluation choices for high-impact results.', 'language': 'en'}, page_content=\"Breakout Agentic Apps\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:\\u200d•\\xa0UX:\\xa0How users interact with their agent• Cognitive architecture: How their agent thinks•\\xa0Prompt engineering: Best practices for prompting• Evaluations: How to gain confidence in agent performance\\n\\n\\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\\n\\n\\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\\n\\n\\nAn AI\\xa0answer engine that lets you handle complex query searches like a (Perplexity) Pro\\n\\n\\nTransforming how users build software from scratch with Replit Agent \\xa0\\n\\n\\nAll systems operationalPrivacy PolicyTerms of Service\\n\\n\\n\\n\\n\")],\n",
       " [Document(metadata={'source': 'https://blog.langchain.com/langchain-state-of-ai-2024', 'title': 'LangChain State of AI 2024 Report', 'description': 'Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving.', 'language': 'en'}, page_content='\\n\\n\\nLangChain State of AI 2024 Report\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangChain State of AI 2024 Report\\nDive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving. \\n\\nBy LangChain\\n6 min read\\nDec 19, 2024\\n\\n\\n\\n\\n\\nAnother year of building with LLMs is coming to an end —\\xa0and 2024 didn’t disappoint. With nearly 30k users signing up for LangSmith every month, we’re lucky to have front row seats to what’s happening in the industry.\\xa0As we did last year, we want to share some product usage patterns that showcase how the AI\\xa0ecosystem and practice of building LLM apps are evolving. As folks have traced, evaluated, and iterated their way around LangSmith, we’ve seen a few notable changes. These include the dramatic rise of open-source model adoption and a shift from predominantly retrieval workflows to AI agent applications with multi-step, agentic workflows.\\xa0Dive into the stats below to learn exactly what developers are building, testing, and prioritizing.Infrastructure usageWith Large Language Models (LLMs) eating the world, everyone’s asking the mirror-mirror-on-the-wall question: “Which model is the most utilized of them all?” Let’s unpack what we’ve seen.Top LLM providersLike last year’s results, OpenAI reigns as the most used LLM provider among LangSmith users —\\xa0used more than 6x as much as Ollama, the next-most popular provider (counted by LangSmith organization usage).Interestingly, Ollama and Groq (which both allow users to run open source models, with the former focusing on local execution and the latter on cloud deployment) have accelerated in momentum this year, breaking into the top 5. This shows a growing interest in more flexible deployment options and customizable AI infrastructure.When it comes to providers that offer open-source models, the top providers have stayed relatively consistent compared to last year - Ollama, Mistral, and Hugging Face have made it easy for developers to run open source models on their platforms. These OSS providers’ collective usage represents 20% of the top 20 LLM providers (by the number of organizations using them).\\xa0Top Retrievers / Vector StoresPerforming retrieval is still critical for many GenAI workflows. The top 3 vector stores have remained the same as last year, with Chroma and FAISS as the most popular choices. This year, Milvus, MongoDB, and Elastic’s vector databases have also entered the top 10.\\xa0Building with LangChain productsAs developers have gained more experience utilizing generative AI, they are also building more dynamic applications. From the growing sophistication of workflows, to the rise of AI agents — we’re seeing a few trends that point to an evolving ecosystem of innovation.Observability isn’t limited to LangChain applicationsWhile langchain (our open source framework) is central to many folks’ LLM app development journeys, 15.7% of LangSmith traces this year come from non-langchain frameworks. This reflects a broader trend where observability is needed regardless of what framework you’re using to build the LLM app — and that interoperability is supported by LangSmith.Python remains dominant, while JavaScript usage growsDebugging, testing, and monitoring certainly has a special place in our Python developers’ hearts, with 84.7% usage coming from the Python SDK. But there is a notable and growing interest in JavaScript as developers pursue web-first applications —\\xa0the JavaScript SDK accounts for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.\\xa0AI agents are gaining tractionAs companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction —\\xa0with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.This growth aligns with the rise in agentic behavior: we see that on average 21.9% of traces now involve tool calls, up from an average of 0.5% in 2023. Tool calling allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action. Increased use of tool calling can enhance an agent’s ability to interact with external systems and perform tasks like writing to databases.\\xa0Performance and optimizationBalancing speed and sophistication is a key challenge when developing applications — especially those leveraging LLM resources. Below, we explore how organizations are interacting with their applications to align the complexity of their needs with efficient performance.Complexity is growing, but tasks are being handled efficiently\\xa0\\xa0The average number of steps per trace has more than doubled over the past year, rising from on average 2.8 steps (2023) to 7.7 steps (2024). We define a step as a distinct operation within a trace, such as a call to an LLM, retriever, or tool. This growth in steps signals that organizations are leveraging more complex and multi-faceted workflows. Rather than a simple question-answer interaction, users are building systems that chain together multiple tasks, such as retrieving information, processing it, and generating actionable results.In contrast, the average number of LLM calls per trace has grown more modestly— from on average 1.1 to 1.4 LLM calls. This speaks to how developers are designing systems to achieve more with fewer LLM calls, balancing functionality while keeping expensive LLM requests in checkLLM testing & evaluationWhat are organizations doing to test their LLM applications to guard against inaccurate or low-quality LLM-generated responses? While it’s no easy feat to keep the quality of your LLM app high, we see organizations using LangSmith’s evaluation capabilities to automate testing and generate user feedback loops to create more robust, reliable applications.LLM-as-Judge: Evaluating what mattersLLM-as-Judge evaluators capture grading rules into an LLM prompt and use the LLM to score whether the output adheres to specific criteria. We see developers testing for these characteristics the most: Relevance, Correctness, Exact Match, and HelpfulnessThese highlight that most developers are doing coarse checks for response quality to make sure AI generated outputs don’t completely miss the mark.\\xa0Iterating with human feedback\\xa0Human feedback is a key part of the iteration loop for folks building LLM apps. LangSmith speeds up the process of collecting and incorporating human feedback on traces and runs (i.e. spans) – so that users can create rich datasets for improvement and optimization. Over the past year, annotated runs grew 18x, scaling linearly with growth in LangSmith usage.Feedback volume per run also increased slightly, rising from 2.28 to 2.59 feedback entries per run. Still, feedback is relatively sparse per run. Users may be prioritizing speed in reviewing runs over providing comprehensive feedback, or commenting on only the most critical or problematic runs that need attention.\\xa0ConclusionIn 2024, developers leaned into complexity with multi-step agents, sharpened efficiency by doing more with fewer LLM calls, and added quality checks to their apps using methods of feedback and evaluation. As more LLM apps are created, we’re excited to see how folks dig into smarter workflows, better performance, and stronger reliability.\\xa0Learn more here about how LangSmith can bring more visibility into your LLM app development and improve performance over time —\\xa0from debugging bottlenecks to evaluating response quality to monitoring regressions.\\xa0\\xa0\\n\\n\\nTags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing End-to-End OpenTelemetry Support in LangSmith\\n\\n\\nBy LangChain\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing OpenTelemetry support for LangSmith\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEasier evaluations with LangSmith SDK v0.2\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph Platform in beta: New deployment options for scalable agent infrastructure\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFew-shot prompting to improve tool-calling performance\\n\\n\\nBy LangChain\\n8 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImproving core tool interfaces and docs in LangChain\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac8cb2ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndoc_list = []\\nfor sublist in docs:\\n    for item in sublist:\\n        doc_list.append(item)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "\"\"\"\n",
    "doc_list = []\n",
    "for sublist in docs:\n",
    "    for item in sublist:\n",
    "        doc_list.append(item)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5deacd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://www.langchain.com/breakoutagents', 'title': 'Breakout Agentic Apps', 'description': 'Dive into the stories of companies pushing the boundaries of AI agents. Learn \"why\" and \"how\" they made specific architecture, UX, prompt engineering, and evaluation choices for high-impact results.', 'language': 'en'}, page_content=\"Breakout Agentic Apps\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:\\u200d•\\xa0UX:\\xa0How users interact with their agent• Cognitive architecture: How their agent thinks•\\xa0Prompt engineering: Best practices for prompting• Evaluations: How to gain confidence in agent performance\\n\\n\\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\\n\\n\\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\\n\\n\\nAn AI\\xa0answer engine that lets you handle complex query searches like a (Perplexity) Pro\"), Document(metadata={'source': 'https://www.langchain.com/breakoutagents', 'title': 'Breakout Agentic Apps', 'description': 'Dive into the stories of companies pushing the boundaries of AI agents. Learn \"why\" and \"how\" they made specific architecture, UX, prompt engineering, and evaluation choices for high-impact results.', 'language': 'en'}, page_content='Navigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\\n\\n\\nAn AI\\xa0answer engine that lets you handle complex query searches like a (Perplexity) Pro\\n\\n\\nTransforming how users build software from scratch with Replit Agent \\xa0\\n\\n\\nAll systems operationalPrivacy PolicyTerms of Service')]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "print(doc_splits[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89a7de64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Breakout Agentic Apps\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Products\n",
      "\n",
      "FrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\n",
      "\n",
      "GuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\n",
      "\n",
      "PythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\n",
      "\n",
      "AboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:‍• UX: How users interact with their agent• Cognitive architecture: How their agent thinks• Prompt engineering: Best practices for prompting• Evaluations: How to gain confidence in agent performance\n",
      "\n",
      "\n",
      "Building an AI tour guide that helps users navigate Ramp's platform for financial operations\n",
      "\n",
      "\n",
      "Navigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\n",
      "\n",
      "\n",
      "An AI answer engine that lets you handle complex query searches like a (Perplexity) Pro' metadata={'source': 'https://www.langchain.com/breakoutagents', 'title': 'Breakout Agentic Apps', 'description': 'Dive into the stories of companies pushing the boundaries of AI agents. Learn \"why\" and \"how\" they made specific architecture, UX, prompt engineering, and evaluation choices for high-impact results.', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(doc_splits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d466541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all text to vector DB\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47d2187b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to multipart ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='a11f71ce-8263-4114-8ff2-a046d7eabbdd', metadata={'source': 'https://blog.langchain.com/langchain-state-of-ai-2024', 'title': 'LangChain State of AI 2024 Report', 'description': 'Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving.', 'language': 'en'}, page_content='for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.\\xa0AI agents are gaining tractionAs companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction —\\xa0with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.This growth aligns with the rise in agentic behavior: we see that on average 21.9% of traces now involve tool calls, up from an average of 0.5% in 2023. Tool calling allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action. Increased use of tool calling can enhance an agent’s ability to interact with external systems and perform tasks like writing to databases.\\xa0Performance and'),\n",
       " Document(id='e6105b83-5323-49da-9901-4375d51b26cf', metadata={'source': 'https://blog.langchain.com/langchain-state-of-ai-2024', 'title': 'LangChain State of AI 2024 Report', 'description': 'Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving.', 'language': 'en'}, page_content='Tags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing End-to-End OpenTelemetry Support in LangSmith\\n\\n\\nBy LangChain\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing OpenTelemetry support for LangSmith\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEasier evaluations with LangSmith SDK v0.2\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph Platform in beta: New deployment options for scalable agent infrastructure\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFew-shot prompting to improve tool-calling performance\\n\\n\\nBy LangChain\\n8 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImproving core tool interfaces and docs in LangChain\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025'),\n",
       " Document(id='614753da-9ec5-49e6-9ea4-ff97a03194a8', metadata={'source': 'https://blog.langchain.com/langchain-state-of-ai-2024', 'title': 'LangChain State of AI 2024 Report', 'description': 'Dive into LangSmith product usage patterns that show how the AI ecosystem and the way people are building LLM apps is evolving.', 'language': 'en'}, page_content='performance, and stronger reliability.\\xa0Learn more here about how LangSmith can bring more visibility into your LLM app development and improve performance over time —\\xa0from debugging bottlenecks to evaluating response quality to monitoring regressions.'),\n",
       " Document(id='f41756bd-621a-435f-8e4b-021597a26bc3', metadata={'source': 'https://www.langchain.com/breakoutagents', 'title': 'Breakout Agentic Apps', 'description': 'Dive into the stories of companies pushing the boundaries of AI agents. Learn \"why\" and \"how\" they made specific architecture, UX, prompt engineering, and evaluation choices for high-impact results.', 'language': 'en'}, page_content=\"Breakout Agentic Apps\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:\\u200d•\\xa0UX:\\xa0How users interact with their agent• Cognitive architecture: How their agent thinks•\\xa0Prompt engineering: Best practices for prompting• Evaluations: How to gain confidence in agent performance\\n\\n\\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\\n\\n\\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\\n\\n\\nAn AI\\xa0answer engine that lets you handle complex query searches like a (Perplexity) Pro\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send compressed multipart ingest: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/multipart in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/multipart', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "retriever.invoke(\"What is LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67fa6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert retriever to RETRIEVER TOOLS\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retriever_vector_db_blog\",\n",
    "    \"A tool to search the LangGraph blog for information\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8ed0efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='retriever_vector_db_blog', description='A tool to search the LangGraph blog for information', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x0000016B1F25CEA0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016B21927560>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x0000016B1F25CFE0>, retriever=VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000016B21927560>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "065f05d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'https://blog.langchain.com/agent-middleware/', 'title': 'Agent Middleware', 'language': 'en'}, page_content='\\n\\n\\nAgent Middleware\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAgent Middleware\\n\\n5 min read\\nSep 8, 2025\\n\\n\\n\\n\\n\\nLangChain has had agent abstractions for nearly three years. There are now probably 100s of agent frameworks with the same core abstraction. They all suffer from the same downsides that the original LangChain agents suffered from: they do not give the developer enough control over context engineering when needed, leading to developers graduating off of the abstraction for any non-trivial use case. In LangChain 1.0 we are introducing a new agent abstraction (Middleware) which we think solves this.The core agent components are quite simple:A modelA promptA list of toolsThe core agent algorithm is equally simple. The user first invokes the agent with some input message, and the agent then runs in a loop, calling tools, adding AI and tool messages to its state, until it decides to not call any tools and ultimately finish.We had a version of this agent in LangChain in November of 2022, and over the past 3 years 100s of frameworks have popped up with similar abstractions.At the same time, while it is simple to get a basic agent abstraction up and running, it is hard to make this abstraction flexible enough to bring to production.In this blog we will cover:Why it is hard to get this abstraction to be reliable enough to bring to productionOur journey to make it more reliable over the past year or soA new Middleware abstraction we are introducing in LangChain 1.0 which we think makes it the most flexible and composable agent abstraction out thereWhy it is hard to bring this abstraction to productionSo why is it still so hard to build reliable agents with these frameworks? Why do many people, when they hit a certain level of complexity, go away from frameworks in favor of custom code?The answer is context engineering. The context that goes into the model determines what comes out of it. In order to make the model (and therefore, the agent) more reliable, you want to have full control over what goes into the model. And while this simple agent state and simple agent loop are great for getting started, as you push the boundaries of your agent’s performance you will likely want to modify part of that.There are a number of things you may want to have more control over as complexity increases:You may want to adjust the “state” of the agent to contain more than just messagesYou may want to have more control over what exactly goes into the modelYou may want to have more control over the sequence of steps runOur journey to make it more reliableOver the past two years we worked to make our agent abstraction better support context engineering. Some things we did (roughly in order):Allowed the user to specify runtime configuration, to pass in things like connection strings and read only user infoAllowed the user to specify arbitrary state schemas, that either the user or the agent could updateAllowed the user to specify a function to return a prompt, rather than a string, allowing for dynamic promptsAllowed the user to specify a function to return a list of messages, to have full control over the whole message list that was sent to the modelAllowed the user to specify a “pre model hook”, to run a step BEFORE the model was called, that could update state or jump to a different node. This allows for things like summarization of long conversations.Allowed the user to specify a “post model hook”, to run a step AFTER the model was called, that could update state or jump to a different node. This allows for things like human-in-the-loop and guardrails.Allowed the user to specify a function that returned the model to use at each call, making it possible to do dynamic model switching and dynamic tool calling.This allowed for high level of customization and control over the context engineering that gets done.But it also resulted in a large number of parameters to the agent. Furthermore, these parameters often had dependencies on each other, which made it tough to coordinate. And it was tough to combine multiple versions of these parameters, or provide off-the-shelf variants to try.What we’re doing in LangChain 1.0For LangChain 1.0 we are leaning to this idea of modifying this core agent loop by introducing a concept of Middleware.The core agent loop will still consist of a model node and a tool node. But Middleware can now specify:before_model: will run before model calls, can update state or jump to other nodes.after_model: will run after model calls, can update state or jump to other nodes.modify_model_request: will run before model calls, allows user to modify (only for that model request) the tools, prompt, message list, model, model settings, output format, and tool choice.You can provide multiple middleware to an agent. They will run as middleware runs in web servers: sequentially on the way in to the model call (before_model, modify_model_request), and in reverse sequential order on the way back (after_model).Middleware can also specify custom state schemas and tools as well.We will provide off-the-shelf middleware for developers to get started with. We will also maintain a list of community middleware for easy access. For a while, developers have asked for collections of nodes to plug into LangGraph agents. This is exactly that.Middleware will also help us unify our different agent abstractions. We currently have separate LangGraph agents for supervisor, swarm, bigtool, deepagents, reflection, and more. We’ve already verified that we will be able replicate these architectures using Middleware.Try it in LangChain 1.0 alphaYou can try out Middleware in the most recent LangChain 1.0 alpha releases (Python and JavaScript). This is the biggest new part of LangChain 1.0 so we would LOVE feedback on Middleware.As part of this alpha release, we are also releasing three different middleware implementations (all of which we are using in internal agents already):Human-in-the-loop: uses Middleware.after_model to provide an off-the-shelf way to add interrupts to get human-in-the-loop feedback on tool calls.Summarization: uses Middleware.before_model to summarize messages once they accumulate past a certain thresholdAnthropic Prompt Caching: uses Middleware.modify_model_request to add special prompt caching tags to messages.Try it in Python: pip install --pre -U langchainTry it in JavaScript: npm install langchain@next\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')],\n",
       " [Document(metadata={'source': 'https://blog.langchain.com/standard-message-content/', 'title': 'Standard message content', 'language': 'en'}, page_content='\\n\\n\\nStandard message content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to content\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\n\\nIn the Loop\\n\\n\\n\\n\\nLangChain\\n\\n\\n\\n\\nDocs\\n\\n\\n\\n\\nChangelog\\n\\n\\n\\n\\n\\nSign in\\nSubscribe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nStandard message content\\n\\n3 min read\\nSep 3, 2025\\n\\n\\n\\n\\n\\nTLDR: We’ve introduced a new view of message content that standardizes reasoning, citations, server-side tool calls, and other modern LLM features across providers. This makes it easier to build applications that are agnostic of the inference provider, while taking advantage of the latest features of each. This feature is fully backward-compatible as it can be computed lazily from existing message content.MotivationOne of LangChain\\'s core strengths is providing a \"write once, run anywhere\" abstraction for large language models. This abstraction allows developers to build applications that can seamlessly switch between different LLM providers without rewriting code.There’s been a recent explosion in the richness and variety of features offered by major inference providers, including OpenAI, Anthropic, and Google Gemini. LLMs can now take multiple distinct steps in service of a query, from laying out their reasoning, to performing web searches and invoking code interpreters, to generating a final response with citations and potentially multimodal data, such as images. Although the set of features supported by each provider is similar, their APIs have diverged, and compatibility layers— such as Chat Completions APIs offered by each provider— typically lag in supporting (or don’t support at all) the full set of provider-native features.Standard content in LangChain 1.0We’ve introduced new standard types for the latest LLM features, and exposed them on all LangChain message objects, making it easier to build provider-agnostic applications without sacrificing support for any available features. These features will be available in langchain 1.0 and are available for both Python and JS. Standard content blocks ensure that identical capabilities are represented identically across providers. In practice, they are a set of typed data structures that normalize message content across all LLM providers. They include:Standard text output from models (including citations)Model reasoning and chain-of-thought outputImages, audio, video, and documents from any source (URL, base64, bucket file ID)Tool/function calls and invocationsProvider-specific tools including built-in web search capabilities and code executionDetailsAll LangChain message objects now implement a .content_blocks property which will lazily load the new representation from the existing message content. Consider results from Anthropic’s Claude and OpenAI’s Responses API. In this example we engage their reasoning and web search features. The raw .content will pass through the provider-native format:Anthropic:from langchain.chat_models import init_chat_model\\n\\nllm = init_chat_model(\\n    \"anthropic:claude-sonnet-4-20250514\",\\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 5_000},\\n).bind_tools([\\n    {\\n        \"type\": \"web_search_20250305\",\\n        \"name\": \"web_search\",\\n        \"max_uses\": 1,\\n    }\\n])\\n\\nresponse = llm.invoke(\"When was LangChain created?\")\\n\\nresponse.content\\n# [\\n#   {\\n#     \"type\": \"thinking\",\\n#     \"thinking\": \"...\",\\n#     \"signature\": \"...\",\\n#   },\\n#   {\\n#     \"type\": \"server_tool_use\",\\n#     \"name\": \"web_search\",\\n#     \"input\": {...},\\n#     \"id\": \"...\",\\n#   },\\n#   {\\n#     \"type\": \"web_search_tool_result\",\\n#     \"content\": [...],\\n#     \"tool_use_id\": \"...\",\\n#   }\\n#   {\\n#     \"type\": \"text\",\\n#     \"text\": \"...\",\\n#     \"citations\": [...],\\n#   }\\nOpenAI:from langchain.chat_models import init_chat_model\\n\\nllm = init_chat_model(\\n    \"openai:gpt-5-nano\",\\n    reasoning={\"effort\": \"low\", \"summary\": \"auto\"},\\n).bind_tools([{\"type\": \"web_search_preview\"}])\\n\\nresponse = llm.invoke(\"When was LangChain created?\")\\n\\nresponse.content\\n# [\\n#   {\\n#     \"type\": \"reasoning\",\\n#     \"summary\": [...],\\n#     \"id\": \"...\",\\n#   },\\n#   {\\n#     \"type\": \"web_search_call\"\\n#     \"action\": {...},\\n#     \"id\": \"...\",\\n#     ...\\n#   },\\n#   {\\n#     \"type\": \"text\",\\n#     \"text\": \"...\",\\n#     \"annotations\": [...],\\n#     \"id\": \"...\",\\n#   }\\nAlthough the content of these responses are similar, small differences in the APIs add compounding friction to building an application that would let you swap between these two providers.The new .content_blocks property will parse both responses into a consistent representation:response.content_blocks\\n# [\\n#   {\\n#     \"type\": \"reasoning\",\\n#     \"reasoning\": \"...\",\\n#   },\\n#   {\\n#     \"type\": \"web_search_call\",\\n#     \"query\": \"...\",\\n#     \"id\": \"...\",\\n#     \"extras\": {...},\\n#   },\\n#   {\\n#     \"type\": \"web_search_result\",\\n#     \"urls\": [...],\\n#     \"id\": \"...\",\\n#     \"extras\": {...},\\n#   },\\n#   {\\n#     \"type\": \"text\",\\n#     \"text\": \"...\",\\n#     \"annotations\": [...],\\n#   }\\n\\nThe output of .content_blocks includes new types for reasoning, citations, web searches, code interpreter calls, and also includes LangChain types for (client side) tool calls and multimodal data, such as images, PDF documents, and audio.Standard content blocks are currently available in alpha forProviders implementing chat completions APIs (including OpenAI)OpenAI Responses APIAnthropic (Claude)langchain 1.0 will feature support for all major providers.Backward compatibilityNo breaking changes; 100% compatible with existing LangChain applications.content_blocks works on all message types, including legacy ones stored in cacheLooking forwardStandard content blocks represent a fundamental step toward more reliable, maintainable LLM applications.By providing consistent interfaces across providers, you can:Build with confidence: Type safety catches errors before productionScale across providers: Switch models without spending time rewriting application logicFuture-proof applications: New provider features work immediately without breaking existing codeReady to try it? Check out our migration guide and technical docs.Questions or feedback? Please comment on the dedicated Github issue for the release. We’d appreciate any thoughts you have to share!\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for langchain Blogs\n",
    "\n",
    "langchain_urls = [\n",
    "    \"https://blog.langchain.com/agent-middleware/\",\n",
    "    \"https://blog.langchain.com/standard-message-content/\"\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in langchain_urls]\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b309abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "doc_splits = text_splitter.split_documents(doc_list)\n",
    "\n",
    "# Add all these text to DB\n",
    "\n",
    "vectorstorelangchain = FAISS.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "retrieverlangchain = vectorstorelangchain.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b77953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool_langchain = create_retriever_tool(\n",
    "    retrieverlangchain,\n",
    "    \"retriever_vector_db_langchain_blog\",\n",
    "    \"A tool to search the Langchain blog for information\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc52311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool, retriever_tool_langchain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef13f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langgraph workflow\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgenticState(TypedDict):\n",
    "    # The add_messages fn defines how an update should be processed\n",
    "    # Default is to replace, add_messages says \"append\"\n",
    "\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfb8f4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQJIi7c78xCTjzntnGqsUxGBxvHGo', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--44e16e9f-c6db-4852-96eb-e0d1fc244292-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a692364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: AgenticState):\n",
    "    \"\"\"\n",
    "    Invokes the agent model to generate a response based on the current static.\n",
    "    Given the question, it will decide to retrive using the retriever tool, or simply end.\n",
    "    \n",
    "    Args:\n",
    "        state (messages): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the agent's response to messages.\n",
    "    \"\"\"\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state[\"messages\"]\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    model = model.bind_tools(tools)\n",
    "    response = model.invoke(messages)\n",
    "    # we return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40ec9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from typing import Literal, Annotated, Sequence\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14c96a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state: AgenticState) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved docs are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state of the conversation.\n",
    "    \n",
    "    Returns:\n",
    "        str: A decision for whether the docs are relevant or not\n",
    "    \"\"\"\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "\n",
    "    # Data Model\n",
    "    class grade(BaseModel):\n",
    "        \"\"\"Binary score for relevance check\"\"\"\n",
    "\n",
    "        binary_score: str = Field(description=\"relevance score 'yes' or 'no'\")\n",
    "\n",
    "    # LLM\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    #LLM with tool & validation\n",
    "    llm_with_tool = model.with_structured_output(grade)\n",
    "\n",
    "    # Prompt\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\" You are a grader assessing relevance of a retrieved documents to a user.\n",
    "        Here is the retrieved documents: \\n\\n {context} \\n\\n\n",
    "        Here is the user question: \\n\\n {question} \\n\\n\n",
    "        If the documents contains keyword(s) or semantic meaning related to the user question,\n",
    "        Give a binary score 'yes' or 'no' to indicate whether the document is relevant or not\n",
    "        \"\"\",\n",
    "        input_variables = [\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    # chain\n",
    "    chain = prompt | llm_with_tool\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "\n",
    "    scored_result = chain.invoke({\"context\": docs, \"question\": question})\n",
    "\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---RELEVANT---\")\n",
    "        return \"generate\"\n",
    "\n",
    "    else:\n",
    "        print(\"---NOT RELEVANT---\")\n",
    "        print(score)\n",
    "        return \"rewrite\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a71287e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: AgenticState):\n",
    "    \"\"\"\n",
    "    Generates a response using the retrieved documents.\n",
    "\n",
    "    Args:\n",
    "        state (messages): The current state of the conversation.\n",
    "\n",
    "    Returns:\n",
    "        dict: The updated state with the generated response to messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"---GENERATE---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    docs = last_message.content\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # chain\n",
    "    rag_chain = prompt | llm| StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0473ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state: AgenticState):\n",
    "    \"\"\"\n",
    "    transform the query to produce a better question.\n",
    "\n",
    "    Args: \n",
    "        state (messages): The current state of the conversation.\n",
    "    \n",
    "    Returns:\n",
    "        dict: The updated state with the rewritten question to messages.\n",
    "    \"\"\"\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    messages = state[\"messages\"]\n",
    "    question = messages[0].content\n",
    "\n",
    "    msg = [\n",
    "        HumanMessage(\n",
    "            content=f\"\"\" \\n\n",
    "    Look at the input & try to reason about the underlying semantic intent/meaning.\n",
    "    Here is the initial question:\n",
    "    \\n-----------\\n\n",
    "    Formulate an improved question:\"\"\",\n",
    "\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Grader\n",
    "    model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    response = model.invoke(msg)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b485aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CALL AGENT---\n",
      "---CHECK RELEVANCE---\n",
      "---RELEVANT---\n",
      "---GENERATE---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is LangGraph?', additional_kwargs={}, response_metadata={}, id='d8b5739d-b008-4206-8577-c9b82b1f7205'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_GRlwejI0hBK8z3C94wyWYBzG', 'function': {'arguments': '{\"query\":\"What is LangGraph?\"}', 'name': 'retriever_vector_db_blog'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 104, 'total_tokens': 126, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CQK6HzMKmaSDzRMZVJSCKDmRV2ET3', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8eba4555-4738-47f1-bbb5-31838b43c7a0-0', tool_calls=[{'name': 'retriever_vector_db_blog', 'args': {'query': 'What is LangGraph?'}, 'id': 'call_GRlwejI0hBK8z3C94wyWYBzG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 104, 'output_tokens': 22, 'total_tokens': 126, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"for 15.3% of LangSmith usage this year, increasing 3x compared to the previous year.\\xa0AI agents are gaining tractionAs companies are getting more serious about incorporating AI agents across various industries, adoption of our controllable agent framework, LangGraph, is also on the rise. Since its release in March 2024, LangGraph has steadily gained traction —\\xa0with 43% of LangSmith organizations are now sending LangGraph traces. These traces represent complex, orchestrated tasks that go beyond basic LLM interactions.This growth aligns with the rise in agentic behavior: we see that on average 21.9% of traces now involve tool calls, up from an average of 0.5% in 2023. Tool calling allows a model to autonomously invoke functions or external resources, signaling more agentic behavior where the model decides when to take action. Increased use of tool calling can enhance an agent’s ability to interact with external systems and perform tasks like writing to databases.\\xa0Performance and\\n\\nTags\\nBy LangChain\\n\\n\\nJoin our newsletter\\nUpdates from the LangChain team and community\\n\\n\\nEnter your email\\n\\nSubscribe\\n\\nProcessing your application...\\nSuccess! Please check your inbox and click the link to confirm your subscription.\\nSorry, something went wrong. Please try again.\\n\\n\\n\\n\\n\\nYou might also like\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing End-to-End OpenTelemetry Support in LangSmith\\n\\n\\nBy LangChain\\n3 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroducing OpenTelemetry support for LangSmith\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEasier evaluations with LangSmith SDK v0.2\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLangGraph Platform in beta: New deployment options for scalable agent infrastructure\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFew-shot prompting to improve tool-calling performance\\n\\n\\nBy LangChain\\n8 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nImproving core tool interfaces and docs in LangChain\\n\\n\\nBy LangChain\\n4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSign up\\n\\n\\n\\n\\n\\n            © LangChain Blog 2025\\n\\nperformance, and stronger reliability.\\xa0Learn more here about how LangSmith can bring more visibility into your LLM app development and improve performance over time —\\xa0from debugging bottlenecks to evaluating response quality to monitoring regressions.\\n\\nBreakout Agentic Apps\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nProducts\\n\\nFrameworksLangGraphLangChainPlatformsLangSmithLangGraph PlatformResources\\n\\nGuidesBlogCustomer StoriesLangChain AcademyCommunityEventsChangelogDocs\\n\\nPythonLangGraphLangSmithLangChainJavaScriptLangGraphLangSmithLangChainCompany\\n\\nAboutCareersPricingGet a demoSign upBreakout Agentic AppsIn this series, dive into the stories of companies pushing the boundaries of AI agents. Learn how each team approached:\\u200d•\\xa0UX:\\xa0How users interact with their agent• Cognitive architecture: How their agent thinks•\\xa0Prompt engineering: Best practices for prompting• Evaluations: How to gain confidence in agent performance\\n\\n\\nBuilding an AI tour guide that helps users navigate Ramp's platform for financial operations\\n\\n\\nNavigate your inbox and calendar in a flash, with an AI-powered search assistant for emails\\n\\n\\nAn AI\\xa0answer engine that lets you handle complex query searches like a (Perplexity) Pro\", name='retriever_vector_db_blog', id='fc8fbe5c-40c0-4454-a2e5-d4e692e3f5b8', tool_call_id='call_GRlwejI0hBK8z3C94wyWYBzG')]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# define a graph\n",
    "workflow = StateGraph(AgenticState)\n",
    "\n",
    "# define the nodes\n",
    "workflow.add_node(\"agent\", agent)\n",
    "retrieve = ToolNode([retriever_tool, retriever_tool_langchain])\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"rewrite\", rewrite)\n",
    "workflow.add_node(\"generate\", generate) # generate a response after we know the docs are relevant\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        # translate the condition outputs to nodes in our graph\n",
    "        \"tools\": \"retrieve\",\n",
    "        END: END, \n",
    "    },\n",
    ")\n",
    "\n",
    "# Edges taken after action node is called\n",
    "workflow.add_conditional_edges(\"retrieve\",\n",
    "    # Assess the agent decision\n",
    "    grade_documents,\n",
    ")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "workflow.add_edge(\"rewrite\", \"agent\")\n",
    "    \n",
    "# compile the graph\n",
    "graph = workflow.compile()\n",
    "\n",
    "# run the graph\n",
    "graph.invoke({\"messages\": [HumanMessage(content=\"What is LangGraph?\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393106d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
