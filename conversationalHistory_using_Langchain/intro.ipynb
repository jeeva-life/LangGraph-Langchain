{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32d0a29",
   "metadata": {},
   "source": [
    "#BUILDING A CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a017600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_groq import ChatGroq \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model_llm = ChatGroq(\n",
    "    model_name=\"gemma2-9b-it\",\n",
    "    api_key=groq_api_key,\n",
    "    temperature=0.5,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292b5077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings, but I'm here and ready to assist you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 15, 'total_tokens': 47, 'completion_time': 0.058181818, 'prompt_time': 0.001259878, 'queue_time': 0.11149958, 'total_time': 0.059441696}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b24e4f25-8b32-459b-9cfb-2d7a83cde1e4-0', usage_metadata={'input_tokens': 15, 'output_tokens': 32, 'total_tokens': 47})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "model_llm.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78680e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin. 🇩🇪  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 40, 'total_tokens': 54, 'completion_time': 0.025454545, 'prompt_time': 0.001580188, 'queue_time': 0.088141131, 'total_time': 0.027034733}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9077dfa8-e314-4b8e-827c-0d5fc7421a39-0', usage_metadata={'input_tokens': 40, 'output_tokens': 14, 'total_tokens': 54})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "model_llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What is the capital of France?\"),\n",
    "        AIMessage(content=\"The capital of France is Paris.\"),\n",
    "        HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f0137",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cf8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816dfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model_llm, get_session_history)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6593ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ee2c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you!  \\n\\nIs there anything I can help you with today? 😊  \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01a93ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's nice to meet you, Jeevan!  \\n\\nWhat can I do for you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 100, 'total_tokens': 124, 'completion_time': 0.043636364, 'prompt_time': 0.002740667, 'queue_time': 0.111655733, 'total_time': 0.046377031}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f733fd3e-ff24-429f-90f4-b155ef3d1371-0', usage_metadata={'input_tokens': 100, 'output_tokens': 24, 'total_tokens': 124})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"My name is Jeevan\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908f8874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Jeevan.  \\n\\nI remember! 😊  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 139, 'total_tokens': 166, 'completion_time': 0.049090909, 'prompt_time': 0.003537086, 'queue_time': 0.089164853, 'total_time': 0.052627995}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9322d219-258d-43ef-94cc-4f1b429c8dbe-0', usage_metadata={'input_tokens': 139, 'output_tokens': 27, 'total_tokens': 166})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, what is my name?\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654238b5",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d03063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant, to the best of your ability\",),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # This is a placeholder for the messages with key, value pair\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7357a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f6b52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings like humans do. But I'm here and ready to assist you! How can I help you today? 😊  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 27, 'total_tokens': 65, 'completion_time': 0.069090909, 'prompt_time': 0.001404339, 'queue_time': 0.111780215, 'total_time': 0.070495248}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6dffc2ae-d7d0-4e02-b08b-0a678a92ac1d-0', usage_metadata={'input_tokens': 27, 'output_tokens': 38, 'total_tokens': 65})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5b16a",
   "metadata": {},
   "source": [
    "# how to invoke with chat message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88549291",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21740524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jeevan! It's nice to meet you. 👋\\n\\nHow can I help you today? 😊  \\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, my name is jeevan\")\n",
    "    ],\n",
    "    config = config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a191603",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of 1 input variable, let's add multiple input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4904bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant, to the best of your ability in {language}\",),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53bd824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'నమస్తే! నేను బాగున్నాను. మీలా? \\n\\n(Namaste! Nenu bagunnanu. Mee la?)\\n\\nThis translates to:\\n\\nHello! I am doing well. How are you?\\n\\n\\nLet me know if you have any other questions. 😊 \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"language\": \"Telugu\",\n",
    "        \"messages\": [HumanMessage(content=\"Hello, how are you?\")]\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d66f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now wrap this chain in a Message History class. this time,\n",
    "# because there are muliplt KEYS in the input, we need to specify the correct key to use \n",
    "# to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cddb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3838aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "521cb76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'నాకు బాగుంది, మీకు ఏమై ఉంది? \\n\\n(Naku bagunndi, meeku ēmaĭ uṇḍi?)\\n\\nThis translates to \"I\\'m doing well, how are you?\" in Telugu. \\n\\nLet me know if you have any other questions or need help with anything else. 😊\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hello, how are you?\")],\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config = config,\n",
    ")\n",
    "response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0424e",
   "metadata": {},
   "source": [
    "# Managing CHAT message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06afdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One important concept to understand when building chatbots is how to manage\n",
    "# conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the \n",
    "# Model.\n",
    "\n",
    "# Therefore, it is important to add a step that imits the size of the messages you are passing in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32f84c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# trim_messages helper to reduce how many messages we're sending to the model.\n",
    "# The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always\n",
    "# keep the system message and whether to allow partial messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39f1fba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Jeevan', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my age?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You are 25 years old', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my favorite color?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your favorite color is blue', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my favorite food?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your favorite food is pizza', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=100,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model_llm,\n",
    "    #include_system_message=True,\n",
    "    allow_partial=True,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
    "    HumanMessage(content=\"What is my name?\"),\n",
    "    AIMessage(content=\"Your name is Jeevan\"),\n",
    "    HumanMessage(content=\"What is my age?\"),\n",
    "    AIMessage(content=\"You are 25 years old\"),\n",
    "    HumanMessage(content=\"What is my favorite color?\"),\n",
    "    AIMessage(content=\"Your favorite color is blue\"),\n",
    "    HumanMessage(content=\"What is my favorite food?\"),\n",
    "    AIMessage(content=\"Your favorite food is pizza\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc7cc814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to personal information about you, including your favorite food. \\n\\nCould you tell me what your favorite food is? 😊  \\n\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in a chainhow do we pass a trimmer?\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model_llm\n",
    ")\n",
    "\n",
    "response =chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What is my favorite food?\")],\n",
    "    \"language\": \"Telugu\"\n",
    "}\n",
    ")\n",
    "response.content\n",
    "\n",
    "# lets now add a new input variable to the chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "534bed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are 25 years old.  \\n\\nI seem to remember that from before! 😊\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What is my age?\")],\n",
    "    \"language\": \"Telugu\"\n",
    "}\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55709e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are 25 years old.  \\n\\nI seem to remember that from before! 😊  Is there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets wrap this in the message history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is my age?\")],\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content\n",
    "\n",
    "# lets now add a new input variable to the chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
