{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d32d0a29",
   "metadata": {},
   "source": [
    "#BUILDING A CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a017600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_groq import ChatGroq \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model_llm = ChatGroq(\n",
    "    model_name=\"gemma2-9b-it\",\n",
    "    api_key=groq_api_key,\n",
    "    temperature=0.5,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "292b5077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings, but I'm here and ready to assist you! How can I help you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 15, 'total_tokens': 47, 'completion_time': 0.058181818, 'prompt_time': 0.001259878, 'queue_time': 0.11149958, 'total_time': 0.059441696}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--b24e4f25-8b32-459b-9cfb-2d7a83cde1e4-0', usage_metadata={'input_tokens': 15, 'output_tokens': 32, 'total_tokens': 47})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "model_llm.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78680e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Germany is Berlin. üá©üá™  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 40, 'total_tokens': 54, 'completion_time': 0.025454545, 'prompt_time': 0.001580188, 'queue_time': 0.088141131, 'total_time': 0.027034733}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9077dfa8-e314-4b8e-827c-0d5fc7421a39-0', usage_metadata={'input_tokens': 40, 'output_tokens': 14, 'total_tokens': 54})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "model_llm.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What is the capital of France?\"),\n",
    "        AIMessage(content=\"The capital of France is Paris.\"),\n",
    "        HumanMessage(content=\"What is the capital of Germany?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f0137",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5cf8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "816dfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(model_llm, get_session_history)\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6593ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91ee2c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm doing well, thank you!  \\n\\nIs there anything I can help you with today? üòä  \\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c01a93ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It's nice to meet you, Jeevan!  \\n\\nWhat can I do for you today?\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 100, 'total_tokens': 124, 'completion_time': 0.043636364, 'prompt_time': 0.002740667, 'queue_time': 0.111655733, 'total_time': 0.046377031}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--f733fd3e-ff24-429f-90f4-b155ef3d1371-0', usage_metadata={'input_tokens': 100, 'output_tokens': 24, 'total_tokens': 124})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"My name is Jeevan\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "908f8874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Jeevan.  \\n\\nI remember! üòä  \\n\\nIs there anything else I can help you with?\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 139, 'total_tokens': 166, 'completion_time': 0.049090909, 'prompt_time': 0.003537086, 'queue_time': 0.089164853, 'total_time': 0.052627995}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9322d219-258d-43ef-94cc-4f1b429c8dbe-0', usage_metadata={'input_tokens': 139, 'output_tokens': 27, 'total_tokens': 166})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, what is my name?\")\n",
    "        \n",
    "    ],\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654238b5",
   "metadata": {},
   "source": [
    "# Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d03063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant, to the best of your ability\",),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"), # This is a placeholder for the messages with key, value pair\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7357a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f6b52de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As an AI, I don't have feelings like humans do. But I'm here and ready to assist you! How can I help you today? üòä  \\n\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 27, 'total_tokens': 65, 'completion_time': 0.069090909, 'prompt_time': 0.001404339, 'queue_time': 0.111780215, 'total_time': 0.070495248}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--6dffc2ae-d7d0-4e02-b08b-0a678a92ac1d-0', usage_metadata={'input_tokens': 27, 'output_tokens': 38, 'total_tokens': 65})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, how are you?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d5b16a",
   "metadata": {},
   "source": [
    "# how to invoke with chat message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88549291",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21740524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jeevan! It's nice to meet you. üëã\\n\\nHow can I help you today? üòä  \\n\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat3\"}}\n",
    "response = with_message_history.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hello, my name is jeevan\")\n",
    "    ],\n",
    "    config = config\n",
    ")\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a191603",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead of 1 input variable, let's add multiple input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4904bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant, to the best of your ability in {language}\",),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53bd824d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞Æ‡∞∏‡±ç‡∞§‡±á! ‡∞®‡±á‡∞®‡±Å ‡∞¨‡∞æ‡∞ó‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å. ‡∞Æ‡±Ä‡∞≤‡∞æ? \\n\\n(Namaste! Nenu bagunnanu. Mee la?)\\n\\nThis translates to:\\n\\nHello! I am doing well. How are you?\\n\\n\\nLet me know if you have any other questions. üòä \\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke(\n",
    "    {\n",
    "        \"language\": \"Telugu\",\n",
    "        \"messages\": [HumanMessage(content=\"Hello, how are you?\")]\n",
    "    }\n",
    ")\n",
    "response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d66f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets now wrap this chain in a Message History class. this time,\n",
    "# because there are muliplt KEYS in the input, we need to specify the correct key to use \n",
    "# to save the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4cddb128",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain, \n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3838aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"chat4\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "521cb76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‡∞®‡∞æ‡∞ï‡±Å ‡∞¨‡∞æ‡∞ó‡±Å‡∞Ç‡∞¶‡∞ø, ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞è‡∞Æ‡±à ‡∞â‡∞Ç‡∞¶‡∞ø? \\n\\n(Naku bagunndi, meeku ƒìmaƒ≠ u·πá·∏çi?)\\n\\nThis translates to \"I\\'m doing well, how are you?\" in Telugu. \\n\\nLet me know if you have any other questions or need help with anything else. üòä\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"Hello, how are you?\")],\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config = config,\n",
    ")\n",
    "response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0424e",
   "metadata": {},
   "source": [
    "# Managing CHAT message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "06afdcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One important concept to understand when building chatbots is how to manage\n",
    "# conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the \n",
    "# Model.\n",
    "\n",
    "# Therefore, it is important to add a step that imits the size of the messages you are passing in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32f84c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "# trim_messages helper to reduce how many messages we're sending to the model.\n",
    "# The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always\n",
    "# keep the system message and whether to allow partial messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "39f1fba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, how are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"I'm doing well, thank you for asking!\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my name?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Jeevan', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my age?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You are 25 years old', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my favorite color?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your favorite color is blue', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is my favorite food?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your favorite food is pizza', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=100,\n",
    "    strategy=\"last\",\n",
    "    token_counter=model_llm,\n",
    "    #include_system_message=True,\n",
    "    allow_partial=True,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"I'm doing well, thank you for asking!\"),\n",
    "    HumanMessage(content=\"What is my name?\"),\n",
    "    AIMessage(content=\"Your name is Jeevan\"),\n",
    "    HumanMessage(content=\"What is my age?\"),\n",
    "    AIMessage(content=\"You are 25 years old\"),\n",
    "    HumanMessage(content=\"What is my favorite color?\"),\n",
    "    AIMessage(content=\"Your favorite color is blue\"),\n",
    "    HumanMessage(content=\"What is my favorite food?\"),\n",
    "    AIMessage(content=\"Your favorite food is pizza\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc7cc814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have access to personal information about you, including your favorite food. \\n\\nCould you tell me what your favorite food is? üòä  \\n\\n\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in a chainhow do we pass a trimmer?\n",
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    | prompt\n",
    "    | model_llm\n",
    ")\n",
    "\n",
    "response =chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What is my favorite food?\")],\n",
    "    \"language\": \"Telugu\"\n",
    "}\n",
    ")\n",
    "response.content\n",
    "\n",
    "# lets now add a new input variable to the chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "534bed43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are 25 years old.  \\n\\nI seem to remember that from before! üòä\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =chain.invoke(\n",
    "    {\n",
    "    \"messages\": messages + [HumanMessage(content=\"What is my age?\")],\n",
    "    \"language\": \"Telugu\"\n",
    "}\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55709e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are 25 years old.  \\n\\nI seem to remember that from before! üòä  Is there anything else I can help you with?\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets wrap this in the message history\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat5\"}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": messages + [HumanMessage(content=\"What is my age?\")],\n",
    "        \"language\": \"Telugu\"\n",
    "    },\n",
    "    config = config\n",
    ")\n",
    "response.content\n",
    "\n",
    "# lets now add a new input variable to the chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef8fa6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
